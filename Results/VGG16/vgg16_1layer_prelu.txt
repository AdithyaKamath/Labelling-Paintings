/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-02-27 15:43:17.584758: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2018-02-27 15:43:17.780867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-27 15:43:17.781186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
totalMemory: 15.90GiB freeMemory: 15.53GiB
2018-02-27 15:43:17.781213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
Size of train: (13678,)
Size of CV: (1710,)
Number of classes: 57
Loading features from files
Finished loading from file
vgg16_load_all.py:83: UserWarning: Update your `PReLU` call to the Keras 2 API: `PReLU(alpha_initializer="he_normal")`
  y = Dense(1024, activation=PReLU(init='he_normal'),kernel_initializer='glorot_normal',kernel_regularizer=regularizers.l2(0.05))(f1)
/usr/local/lib/python2.7/dist-packages/keras/activations.py:115: UserWarning: Do not pass a layer instance (such as PReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Training Now
Train on 13678 samples, validate on 1710 samples
Epoch 1/50
13678/13678 [==============================] - 9s 676us/step - loss: 45.1632 - acc: 0.1782 - top_k_categorical_accuracy: 0.3515 - val_loss: 29.8455 - val_acc: 0.2877 - val_top_k_categorical_accuracy: 0.4947
Epoch 2/50
13678/13678 [==============================] - 6s 429us/step - loss: 24.0314 - acc: 0.3509 - top_k_categorical_accuracy: 0.5640 - val_loss: 20.2342 - val_acc: 0.3819 - val_top_k_categorical_accuracy: 0.6023
Epoch 3/50
13678/13678 [==============================] - 6s 432us/step - loss: 17.0143 - acc: 0.4682 - top_k_categorical_accuracy: 0.6768 - val_loss: 15.5263 - val_acc: 0.4404 - val_top_k_categorical_accuracy: 0.6789
Epoch 4/50
13678/13678 [==============================] - 6s 430us/step - loss: 13.2980 - acc: 0.5337 - top_k_categorical_accuracy: 0.7378 - val_loss: 12.8214 - val_acc: 0.4877 - val_top_k_categorical_accuracy: 0.7158
Epoch 5/50
13678/13678 [==============================] - 6s 434us/step - loss: 11.1438 - acc: 0.5840 - top_k_categorical_accuracy: 0.7789 - val_loss: 11.4592 - val_acc: 0.4807 - val_top_k_categorical_accuracy: 0.7339
Epoch 6/50
13678/13678 [==============================] - 6s 432us/step - loss: 9.6247 - acc: 0.6139 - top_k_categorical_accuracy: 0.8082 - val_loss: 10.9148 - val_acc: 0.4643 - val_top_k_categorical_accuracy: 0.7287
Epoch 7/50
13678/13678 [==============================] - 6s 433us/step - loss: 8.6915 - acc: 0.6348 - top_k_categorical_accuracy: 0.8212 - val_loss: 10.1382 - val_acc: 0.4842 - val_top_k_categorical_accuracy: 0.7292
Epoch 8/50
13678/13678 [==============================] - 6s 432us/step - loss: 8.0014 - acc: 0.6502 - top_k_categorical_accuracy: 0.8328 - val_loss: 9.5850 - val_acc: 0.4988 - val_top_k_categorical_accuracy: 0.7333
Epoch 9/50
13678/13678 [==============================] - 6s 427us/step - loss: 7.4460 - acc: 0.6675 - top_k_categorical_accuracy: 0.8467 - val_loss: 9.2455 - val_acc: 0.5070 - val_top_k_categorical_accuracy: 0.7439
Epoch 10/50
13678/13678 [==============================] - 6s 428us/step - loss: 7.0957 - acc: 0.6774 - top_k_categorical_accuracy: 0.8471 - val_loss: 9.0955 - val_acc: 0.5000 - val_top_k_categorical_accuracy: 0.7474
Epoch 11/50
13678/13678 [==============================] - 6s 428us/step - loss: 6.6571 - acc: 0.6912 - top_k_categorical_accuracy: 0.8582 - val_loss: 8.3980 - val_acc: 0.5240 - val_top_k_categorical_accuracy: 0.7596
Epoch 12/50
13678/13678 [==============================] - 6s 424us/step - loss: 6.4383 - acc: 0.6961 - top_k_categorical_accuracy: 0.8635 - val_loss: 8.5243 - val_acc: 0.5076 - val_top_k_categorical_accuracy: 0.7573
Epoch 13/50
13678/13678 [==============================] - 6s 428us/step - loss: 6.1924 - acc: 0.7040 - top_k_categorical_accuracy: 0.8691 - val_loss: 8.3292 - val_acc: 0.5012 - val_top_k_categorical_accuracy: 0.7491
Epoch 14/50
13678/13678 [==============================] - 6s 421us/step - loss: 6.1856 - acc: 0.6964 - top_k_categorical_accuracy: 0.8699 - val_loss: 8.1691 - val_acc: 0.4953 - val_top_k_categorical_accuracy: 0.7567
Epoch 15/50
13678/13678 [==============================] - 6s 424us/step - loss: 6.0615 - acc: 0.7039 - top_k_categorical_accuracy: 0.8742 - val_loss: 8.2314 - val_acc: 0.5029 - val_top_k_categorical_accuracy: 0.7357
Epoch 16/50
13678/13678 [==============================] - 6s 418us/step - loss: 5.9445 - acc: 0.7051 - top_k_categorical_accuracy: 0.8760 - val_loss: 8.0767 - val_acc: 0.4953 - val_top_k_categorical_accuracy: 0.7585
Epoch 17/50
13678/13678 [==============================] - 6s 414us/step - loss: 5.8568 - acc: 0.7089 - top_k_categorical_accuracy: 0.8813 - val_loss: 8.1545 - val_acc: 0.5053 - val_top_k_categorical_accuracy: 0.7433
Epoch 18/50
13678/13678 [==============================] - 6s 428us/step - loss: 5.7382 - acc: 0.7155 - top_k_categorical_accuracy: 0.8810 - val_loss: 7.9834 - val_acc: 0.5082 - val_top_k_categorical_accuracy: 0.7520
Epoch 19/50
13678/13678 [==============================] - 6s 432us/step - loss: 5.6553 - acc: 0.7152 - top_k_categorical_accuracy: 0.8847 - val_loss: 7.8111 - val_acc: 0.5205 - val_top_k_categorical_accuracy: 0.7637
Epoch 20/50
13678/13678 [==============================] - 6s 430us/step - loss: 5.5857 - acc: 0.7201 - top_k_categorical_accuracy: 0.8849 - val_loss: 7.8676 - val_acc: 0.5094 - val_top_k_categorical_accuracy: 0.7526
Epoch 21/50
13678/13678 [==============================] - 6s 429us/step - loss: 5.5460 - acc: 0.7174 - top_k_categorical_accuracy: 0.8850 - val_loss: 7.9114 - val_acc: 0.5117 - val_top_k_categorical_accuracy: 0.7556
Epoch 22/50
13678/13678 [==============================] - 6s 427us/step - loss: 5.4891 - acc: 0.7209 - top_k_categorical_accuracy: 0.8852 - val_loss: 8.1636 - val_acc: 0.4912 - val_top_k_categorical_accuracy: 0.7327
Epoch 23/50
13678/13678 [==============================] - 6s 432us/step - loss: 5.5219 - acc: 0.7185 - top_k_categorical_accuracy: 0.8849 - val_loss: 7.9021 - val_acc: 0.5140 - val_top_k_categorical_accuracy: 0.7433
Epoch 24/50
13678/13678 [==============================] - 6s 430us/step - loss: 5.3813 - acc: 0.7253 - top_k_categorical_accuracy: 0.8887 - val_loss: 7.9585 - val_acc: 0.4947 - val_top_k_categorical_accuracy: 0.7392
Epoch 25/50
13678/13678 [==============================] - 6s 432us/step - loss: 5.3935 - acc: 0.7234 - top_k_categorical_accuracy: 0.8882 - val_loss: 8.0669 - val_acc: 0.4906 - val_top_k_categorical_accuracy: 0.7363
Epoch 26/50
13678/13678 [==============================] - 6s 433us/step - loss: 5.3205 - acc: 0.7306 - top_k_categorical_accuracy: 0.8898 - val_loss: 8.0144 - val_acc: 0.4988 - val_top_k_categorical_accuracy: 0.7333