/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-02-27 13:15:53.755268: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2018-02-27 13:15:53.873091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-27 13:15:53.873383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
totalMemory: 15.90GiB freeMemory: 15.51GiB
2018-02-27 13:15:53.873407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
Size of train: (13678,)
Size of CV: (1710,)
Number of classes: 57
Loading train images
Generating Features for train
(13678, 224, 224, 3)
Feature generation complete
Loading test images
Generating Features for test
(1710, 224, 224, 3)
Feature generation complete
((13678, 7, 7, 512), (13678, 57))
((1710, 7, 7, 512), (1710, 57))
Training Now
Train on 13678 samples, validate on 1710 samples
Epoch 1/50
13678/13678 [==============================] - 8s 609us/step - loss: 40.2579 - acc: 0.2162 - top_k_categorical_accuracy: 0.4188 - val_loss: 20.8755 - val_acc: 0.3883 - val_top_k_categorical_accuracy: 0.6251
Epoch 2/50
13678/13678 [==============================] - 6s 423us/step - loss: 13.6648 - acc: 0.4294 - top_k_categorical_accuracy: 0.6790 - val_loss: 8.5160 - val_acc: 0.4778 - val_top_k_categorical_accuracy: 0.7205
Epoch 3/50
13678/13678 [==============================] - 6s 417us/step - loss: 6.9459 - acc: 0.5529 - top_k_categorical_accuracy: 0.8128 - val_loss: 6.0758 - val_acc: 0.5041 - val_top_k_categorical_accuracy: 0.7737
Epoch 4/50
13678/13678 [==============================] - 6s 419us/step - loss: 5.1255 - acc: 0.6233 - top_k_categorical_accuracy: 0.8764 - val_loss: 5.4928 - val_acc: 0.5228 - val_top_k_categorical_accuracy: 0.7807
Epoch 5/50
13678/13678 [==============================] - 6s 413us/step - loss: 4.2650 - acc: 0.6691 - top_k_categorical_accuracy: 0.9070 - val_loss: 4.9690 - val_acc: 0.5415 - val_top_k_categorical_accuracy: 0.8070
Epoch 6/50
13678/13678 [==============================] - 6s 409us/step - loss: 3.7508 - acc: 0.6955 - top_k_categorical_accuracy: 0.9174 - val_loss: 4.8813 - val_acc: 0.5357 - val_top_k_categorical_accuracy: 0.7924
Epoch 7/50
13678/13678 [==============================] - 6s 414us/step - loss: 3.5376 - acc: 0.7047 - top_k_categorical_accuracy: 0.9266 - val_loss: 4.6205 - val_acc: 0.5474 - val_top_k_categorical_accuracy: 0.8082
Epoch 8/50
13678/13678 [==============================] - 6s 413us/step - loss: 3.2724 - acc: 0.7201 - top_k_categorical_accuracy: 0.9376 - val_loss: 4.5721 - val_acc: 0.5503 - val_top_k_categorical_accuracy: 0.8058
Epoch 9/50
13678/13678 [==============================] - 6s 417us/step - loss: 3.1112 - acc: 0.7314 - top_k_categorical_accuracy: 0.9409 - val_loss: 4.3962 - val_acc: 0.5637 - val_top_k_categorical_accuracy: 0.8158
Epoch 10/50
13678/13678 [==============================] - 6s 411us/step - loss: 3.0253 - acc: 0.7349 - top_k_categorical_accuracy: 0.9436 - val_loss: 4.6725 - val_acc: 0.5281 - val_top_k_categorical_accuracy: 0.7947
Epoch 11/50
13678/13678 [==============================] - 6s 412us/step - loss: 2.9545 - acc: 0.7389 - top_k_categorical_accuracy: 0.9491 - val_loss: 4.2262 - val_acc: 0.5585 - val_top_k_categorical_accuracy: 0.8094
Epoch 12/50
13678/13678 [==============================] - 6s 403us/step - loss: 2.8688 - acc: 0.7495 - top_k_categorical_accuracy: 0.9504 - val_loss: 4.5315 - val_acc: 0.5427 - val_top_k_categorical_accuracy: 0.8029
Epoch 13/50
13678/13678 [==============================] - 5s 401us/step - loss: 2.8338 - acc: 0.7536 - top_k_categorical_accuracy: 0.9502 - val_loss: 4.4448 - val_acc: 0.5357 - val_top_k_categorical_accuracy: 0.8146
Epoch 14/50
13678/13678 [==============================] - 5s 399us/step - loss: 2.8083 - acc: 0.7491 - top_k_categorical_accuracy: 0.9526 - val_loss: 4.5525 - val_acc: 0.5281 - val_top_k_categorical_accuracy: 0.8094
Epoch 15/50
13678/13678 [==============================] - 6s 403us/step - loss: 2.7591 - acc: 0.7542 - top_k_categorical_accuracy: 0.9546 - val_loss: 4.4049 - val_acc: 0.5556 - val_top_k_categorical_accuracy: 0.8135
Epoch 16/50
13678/13678 [==============================] - 5s 399us/step - loss: 2.7447 - acc: 0.7536 - top_k_categorical_accuracy: 0.9548 - val_loss: 4.2952 - val_acc: 0.5573 - val_top_k_categorical_accuracy: 0.8146
Epoch 00016: early stopping
Training Complete
Saving Model